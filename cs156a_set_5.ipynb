{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 5"
      ],
      "metadata": {
        "id": "xb9C5WKyCipL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTo-IeyKCghU",
        "outputId": "c4533d76-a0f4-441f-900a-5e419a2cebb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations: 10\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def E(u,v): #define error surface\n",
        "  return (u * np.exp(v) - 2 * v * np.exp(-u)) ** 2\n",
        "\n",
        "def grad_E(u,v): #calculate the gradient of E\n",
        "  chain = u * np.exp(v) - 2 * v * np.exp(-u)\n",
        "  dE_du = 2 * chain * (np.exp(v) + 2 * v * np.exp(-u)) #partial with respect to u\n",
        "  dE_dv = 2 * chain * (u * np.exp(v) - 2 * np.exp(-u)) #partial with respect to v\n",
        "  return dE_du, dE_dv\n",
        "\n",
        "u, v = 1.0, 1.0 #set u and v\n",
        "eta = 0.1 #set eta\n",
        "target_error = 1e-14 #target error is 10^-14\n",
        "iterations = 0\n",
        "\n",
        "while (True):\n",
        "  error = E(u,v)\n",
        "  if (error < target_error): #break if error is less than the target error\n",
        "    break\n",
        "  dE_du, dE_dv = grad_E(u, v) #find the gradient\n",
        "  u -= eta * dE_du #calculate new u and v values after gradient descent\n",
        "  v -= eta * dE_dv\n",
        "  iterations += 1\n",
        "\n",
        "print(f'Iterations: {iterations}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 6"
      ],
      "metadata": {
        "id": "IZ7RU-FKE50r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'(u, v) = {u}, {v}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1xgdG3VE4Td",
        "outputId": "9bf2fe9e-3815-4d29-e2df-c1236e540143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(u, v) = 0.04473629039778207, 0.023958714099141746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 7"
      ],
      "metadata": {
        "id": "VLrU38McE94h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 15 #15 full iterations or 30 steps\n",
        "u, v = 1.0, 1.0\n",
        "eta = 0.1\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  dE_du, dE_dv = grad_E(u, v)\n",
        "  u -= eta * dE_du #step 1, adjusting u for gradient descent\n",
        "  error = E(u,v)\n",
        "  dE_du, dE_dv = grad_E(u, v)\n",
        "  v -= eta * dE_dv #step 2, adjusting v for gradient descent\n",
        "  error = E(u,v) #calculate new error\n",
        "\n",
        "print(f'Error after {num_iterations} iterations: {error}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfVCFKhGFCSx",
        "outputId": "5a28ed11-aa42-4d80-b4c4-5de65a9cef11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error after 15 iterations: 0.13981379199615315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 8"
      ],
      "metadata": {
        "id": "6OCPK8UZE4iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import sched_getscheduler\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "space = [-1, 1]\n",
        "N = 100 #number of training points\n",
        "iterations = 100\n",
        "nu = 0.01 #learning rate\n",
        "test_size = 10000\n",
        "num_epochs = 100\n",
        "\n",
        "\n",
        "def generate_line():\n",
        "  p1 = np.random.uniform(*space, 2)\n",
        "  p2 = np.random.uniform(*space, 2)\n",
        "  slope = (p2[1] - p1[1]) / (p2[0] - p1[0]) if p2[0] != p1[0] else np.inf\n",
        "  b = p1[1] - slope * p1[0]\n",
        "  return lambda x: slope * x + b\n",
        "\n",
        "def prepare_data(N, target_function):\n",
        "  def target(point):\n",
        "      return target_function(point[0])\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "  for i in range(N):\n",
        "    new_pt = np.random.uniform(*space, 2)  #generate random points\n",
        "    y = target(new_pt)\n",
        "    difference = y - new_pt[1]\n",
        "\n",
        "    X.append(new_pt)\n",
        "    Y.append(difference)\n",
        "\n",
        "  targets = np.sign(Y)  #convert differences to -1 or 1\n",
        "  return X, targets\n",
        "\n",
        "def sgd(X, y, nu, num_epochs):\n",
        "  model = SGDClassifier(loss = \"log_loss\", learning_rate=\"constant\", eta0=nu, max_iter=1, warm_start=True)\n",
        "  for epoch in range(num_epochs):\n",
        "    model.partial_fit(X, y, classes=np.array([-1, 1]))\n",
        "  return model\n",
        "\n",
        "def calc_e_out(model, test_size, target_function):\n",
        "  X_test, y_test = prepare_data(test_size, target_function)\n",
        "  y_prob = model.predict_proba(X_test)\n",
        "  return log_loss(y_test, y_prob)\n",
        "\n",
        "\n",
        "e_out_total = 0\n",
        "target_function = generate_line()\n",
        "for run in range(iterations):\n",
        "  X_train, y_train = prepare_data(N, target_function)\n",
        "  model = sgd(X_train, y_train, nu, num_epochs)\n",
        "  e_out = calc_e_out(model, test_size, target_function)\n",
        "  e_out_total += e_out\n",
        "\n",
        "avg_e_out = e_out_total / iterations\n",
        "\n",
        "print(f'Average E_out: {avg_e_out}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5IVW8BWGfJl",
        "outputId": "ce4fe2bc-162e-4ef4-8b6d-ec5412fd53be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average E_out: 0.16323845994612346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the space for random sampling\n",
        "space = [-1, 1]\n",
        "\n",
        "# Function to define a random boundary line\n",
        "def generate_line():\n",
        "    p1 = np.random.uniform(*space, 2)\n",
        "    p2 = np.random.uniform(*space, 2)\n",
        "    slope = (p2[1] - p1[1]) / (p2[0] - p1[0]) if p2[0] != p1[0] else np.inf\n",
        "    b = p1[1] - slope * p1[0]\n",
        "    return lambda x: slope * x + b\n",
        "\n",
        "# Prepare training data based on the generated line\n",
        "def prepare_data(N, target_function):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(N):\n",
        "        new_pt = np.random.uniform(*space, 2)\n",
        "        y = target_function(new_pt[0])\n",
        "        difference = y - new_pt[1]\n",
        "        X.append(new_pt)\n",
        "        Y.append(difference)\n",
        "\n",
        "    targets = np.sign(Y)\n",
        "    return np.array(X), targets\n",
        "\n",
        "# Logistic function\n",
        "def logistic_func(z):\n",
        "    \"\"\"Logistic function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "#train logistic regression using stochastic gradient descent\n",
        "def train_logistic_regression(X, y, learning_rate=0.01, tol=0.01, max_iter=1000):\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    N = len(y)\n",
        "\n",
        "    for epoch in range(max_iter):\n",
        "        weights_prev = weights.copy()\n",
        "        #random permutation of indices for SGD\n",
        "        indices = np.random.permutation(N)\n",
        "        for i in indices:\n",
        "            z = np.dot(X[i], weights)\n",
        "            prediction = logistic_func(z)\n",
        "            error = y[i] - prediction\n",
        "            weights += learning_rate * error * X[i]  #update weights\n",
        "\n",
        "        #check for convergence\n",
        "        if np.linalg.norm(weights - weights_prev) < tol:\n",
        "            break\n",
        "\n",
        "    return weights\n",
        "\n",
        "#calculate cross-entropy loss\n",
        "def cross_entropy_loss(X, y, weights):\n",
        "    predictions = logistic_func(np.dot(X, weights))\n",
        "    return -np.mean(y * np.log(predictions + 1e-15) + (1 - y) * np.log(1 - predictions + 1e-15))\n",
        "\n",
        "# Evaluate model performance on a separate test dataset\n",
        "def evaluate_model(weights, num_test_points=1000, target_function=None):\n",
        "    X_test = np.random.uniform(*space, (num_test_points, 2))\n",
        "    #generate test labels based on the target function\n",
        "    Y_test = np.sign(target_function(X_test[:, 0]) - X_test[:, 1])\n",
        "    predictions = np.where(logistic_func(np.dot(X_test, weights)) >= 0.5, 1, -1)\n",
        "    return np.mean(predictions != Y_test)  #error rate\n",
        "\n",
        "\n",
        "N = 100  #number of training points\n",
        "num_runs = 100  #number of experiments\n",
        "errors = []\n",
        "\n",
        "target_function = generate_line()\n",
        "for _ in range(num_runs):\n",
        "    X, Y = prepare_data(N, target_function)\n",
        "    weights = train_logistic_regression(X, Y)  #train the model\n",
        "    E_out = evaluate_model(weights, target_function=target_function)  #evaluate the model\n",
        "    errors.append(E_out)\n",
        "\n",
        "average_error = np.mean(errors)\n",
        "print(f\"Average E_out over {num_runs} runs: {average_error:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv-i-m_J0m5D",
        "outputId": "893c1e31-6630-4e4c-e4c0-0a3b1d9f8303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average E_out over 100 runs: 0.152280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the space for random sampling\n",
        "space = [-1, 1]\n",
        "\n",
        "def generate_line():\n",
        "    \"\"\"Generate a line based on two random points.\"\"\"\n",
        "    p1 = np.random.uniform(*space, 2)\n",
        "    p2 = np.random.uniform(*space, 2)\n",
        "    slope = (p2[1] - p1[1]) / (p2[0] - p1[0]) if p2[0] != p1[0] else np.inf\n",
        "    b = p1[1] - slope * p1[0]\n",
        "    return lambda x: slope * x + b\n",
        "\n",
        "def prepare_data(N, target_function):\n",
        "    \"\"\"Prepare random data points and their corresponding labels.\"\"\"\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(N):\n",
        "        new_pt = np.random.uniform(*space, 2)  # Generate random points\n",
        "        y = target_function(new_pt[0])  # Get the y-value from the target function\n",
        "        difference = y - new_pt[1]  # Compute difference from line\n",
        "        X.append(new_pt)\n",
        "        Y.append(difference)\n",
        "\n",
        "    targets = np.sign(Y)  # Convert differences to -1 or 1\n",
        "    return np.array(X), targets\n",
        "\n",
        "def logistic_func(z):\n",
        "    \"\"\"Logistic function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def train_logistic_regression(X, y, learning_rate=0.01, tol=0.01, max_iter=10000):\n",
        "    \"\"\"Train logistic regression using Stochastic Gradient Descent.\"\"\"\n",
        "    weights = np.zeros(X.shape[1])  # Initialize weights\n",
        "    N = len(y)  # Number of samples\n",
        "    epoch_count = 0  # To count the number of epochs\n",
        "\n",
        "    for epoch in range(max_iter):\n",
        "        epoch_count += 1\n",
        "        # Random permutation of indices\n",
        "        indices = np.random.permutation(N)\n",
        "        for i in indices:\n",
        "            z = np.dot(X[i], weights)  # Compute linear combination\n",
        "            prediction = logistic_func(z)  # Apply logistic function\n",
        "            error = y[i] - prediction  # Calculate error\n",
        "            weights += learning_rate * error * X[i]  # Update weights\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(weights) < tol:\n",
        "            break\n",
        "\n",
        "    return epoch_count\n",
        "\n",
        "def main(num_runs=10, N=100):\n",
        "    \"\"\"Run multiple experiments to measure average epochs for convergence.\"\"\"\n",
        "    epoch_counts = []\n",
        "\n",
        "    for _ in range(num_runs):\n",
        "        target_function = generate_line()  # Generate a new target line\n",
        "        X, Y = prepare_data(N, target_function)  # Prepare data\n",
        "        epochs = train_logistic_regression(X, Y)  # Train the model\n",
        "        epoch_counts.append(epochs)  # Store the epoch count for this run\n",
        "\n",
        "    average_epochs = np.mean(epoch_counts)  # Calculate average epochs\n",
        "    print(f\"Average number of epochs for convergence: {average_epochs:.2f}\")\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuDaH4xv2SZj",
        "outputId": "ed976a44-c521-4d05-a77b-a3b07f5cc615"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-f423f5b032e7>:30: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of epochs for convergence: 10000.00\n"
          ]
        }
      ]
    }
  ]
}