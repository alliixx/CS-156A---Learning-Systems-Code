{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 5"
      ],
      "metadata": {
        "id": "xb9C5WKyCipL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTo-IeyKCghU",
        "outputId": "c4533d76-a0f4-441f-900a-5e419a2cebb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations: 10\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def E(u,v): #define error surface\n",
        "  return (u * np.exp(v) - 2 * v * np.exp(-u)) ** 2\n",
        "\n",
        "def grad_E(u,v): #calculate the gradient of E\n",
        "  chain = u * np.exp(v) - 2 * v * np.exp(-u)\n",
        "  dE_du = 2 * chain * (np.exp(v) + 2 * v * np.exp(-u)) #partial with respect to u\n",
        "  dE_dv = 2 * chain * (u * np.exp(v) - 2 * np.exp(-u)) #partial with respect to v\n",
        "  return dE_du, dE_dv\n",
        "\n",
        "u, v = 1.0, 1.0 #set u and v\n",
        "eta = 0.1 #set eta\n",
        "target_error = 1e-14 #target error is 10^-14\n",
        "iterations = 0\n",
        "\n",
        "while (True):\n",
        "  error = E(u,v)\n",
        "  if (error < target_error): #break if error is less than the target error\n",
        "    break\n",
        "  dE_du, dE_dv = grad_E(u, v) #find the gradient\n",
        "  u -= eta * dE_du #calculate new u and v values after gradient descent\n",
        "  v -= eta * dE_dv\n",
        "  iterations += 1\n",
        "\n",
        "print(f'Iterations: {iterations}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 6"
      ],
      "metadata": {
        "id": "IZ7RU-FKE50r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'(u, v) = {u}, {v}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1xgdG3VE4Td",
        "outputId": "9bf2fe9e-3815-4d29-e2df-c1236e540143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(u, v) = 0.04473629039778207, 0.023958714099141746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 7"
      ],
      "metadata": {
        "id": "VLrU38McE94h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_iterations = 15 #15 full iterations or 30 steps\n",
        "u, v = 1.0, 1.0\n",
        "eta = 0.1\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "  dE_du, dE_dv = grad_E(u, v)\n",
        "  u -= eta * dE_du #step 1, adjusting u for gradient descent\n",
        "  error = E(u,v)\n",
        "  dE_du, dE_dv = grad_E(u, v)\n",
        "  v -= eta * dE_dv #step 2, adjusting v for gradient descent\n",
        "  error = E(u,v) #calculate new error\n",
        "\n",
        "print(f'Error after {num_iterations} iterations: {error}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfVCFKhGFCSx",
        "outputId": "5a28ed11-aa42-4d80-b4c4-5de65a9cef11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error after 15 iterations: 0.13981379199615315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PROBLEM 8"
      ],
      "metadata": {
        "id": "6OCPK8UZE4iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "N = 100 #number of training points\n",
        "nu = 0.01 #learning rate nu\n",
        "epsilon = 0.01 #convergance error\n",
        "space = (-1, 1)  #define the space for generating points\n",
        "\n",
        "def generate_line(): #randomly generate line by picking two points\n",
        "    p1 = np.random.uniform(*space, 2)\n",
        "    p2 = np.random.uniform(*space, 2)\n",
        "    slope = (p2[1] - p1[1]) / (p2[0] - p1[0]) if p2[0] != p1[0] else np.inf\n",
        "    b = p1[1] - slope * p1[0]\n",
        "    return slope, b\n",
        "\n",
        "def target_function(x, slope, b): #find the correct sign/label for points\n",
        "    return 1 if x[1] > (slope * x[0] + b) else -1\n",
        "\n",
        "def generate_training_data(N, slope, b): #randomly generate training data\n",
        "    data = []\n",
        "    for _ in range(N):\n",
        "        x = np.random.uniform(-1, 1, 2)\n",
        "        y = target_function(x, slope, b) #label the points correctly\n",
        "        data.append((x, y))\n",
        "    return data\n",
        "\n",
        "#calculate the gradient: -(y_n * x_n)/(1 + e^{-y_n * w^T * x_n})\n",
        "def gradient(s, y, w):\n",
        "    vec = [y * 1.0] + [y * x * 1.0 for x in s]\n",
        "    x_n = np.array([1] + list(s))  #include the bias term\n",
        "    new_w = np.array(w) #weight vector\n",
        "    d = (1.0 + np.exp(y * np.dot(x_n, new_w)))  #denominator of gradient\n",
        "    return [-1.0 * x / d for x in vec]\n",
        "\n",
        "def converged(v1, v2, epsilon): #test if weights converged\n",
        "    return np.linalg.norm(np.array(v2) - np.array(v1)) < epsilon\n",
        "\n",
        "#function to update weights: w(t+1) = w(t) - nu * gradient\n",
        "def update_weights(w, g, nu):\n",
        "    return [w[i] - nu * g[i] for i in range(len(w))]\n",
        "\n",
        "#function to perform stochastic gradient descent\n",
        "def sgd(data, epsilon, nu):\n",
        "    w = [0] * (len(data[0][0]) + 1)  #including bias term\n",
        "    converge = False\n",
        "    epochs = 0\n",
        "    idx_order = list(range(len(data)))\n",
        "\n",
        "    while not converge:\n",
        "        old_w = np.copy(w)\n",
        "        np.random.shuffle(idx_order)  #shuffle the index order\n",
        "        for i in idx_order:\n",
        "            s = data[i][0]\n",
        "            y = data[i][1]\n",
        "            grad = gradient(s, y, w)\n",
        "            w = update_weights(w, grad, nu)\n",
        "        epochs += 1\n",
        "        converge = converged(w, old_w, epsilon)\n",
        "    return epochs, w\n",
        "\n",
        "#calculate cross entropy error: log(1 + e^{-y_n * w^T * x_n})\n",
        "def cross_entropy_error(sample, y, w):\n",
        "    x_n = np.array([1] + list(sample))  #include the bias term\n",
        "    new_w = np.array(w)\n",
        "    return np.log(1.0 + np.exp(-y * np.dot(x_n, new_w)))\n",
        "\n",
        "#calculate average E_out: 1/N * sum cross entropy error\n",
        "def compute_E_out(data, w):\n",
        "    ce = 0\n",
        "    for d in data:\n",
        "        x, y = d[0], d[1]\n",
        "        ce += cross_entropy_error(x, y, w)\n",
        "    return ce / len(data)\n",
        "\n",
        "total_epochs = 0\n",
        "total_E_out = 0\n",
        "runs = 100 #avg over 100 runs\n",
        "\n",
        "for _ in range(runs): #run over 100 runs\n",
        "    slope, b = generate_line()\n",
        "    training_data = generate_training_data(N, slope, b)\n",
        "    epochs, w = sgd(training_data, epsilon, nu)\n",
        "    out_sample_data = generate_training_data(10000, slope, b)\n",
        "    E_out = compute_E_out(out_sample_data, w)\n",
        "    total_epochs += epochs\n",
        "    total_E_out += E_out\n",
        "\n",
        "avg_epochs = total_epochs / runs #find avg number of epochs (prob 9)\n",
        "avg_E_out = total_E_out / runs #find avg E_out result over 100 runs (prob 8)\n",
        "print(f'Avg epochs: {avg_epochs}')\n",
        "print(f'Avg E_out: {avg_E_out}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "-z2yQPSb4591",
        "outputId": "dcaa70cd-a953-4384-e3e3-f5e00cbb1c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(array([0.02349469, 0.4328599 ]), 1), (array([0.42950064, 0.78801388]), 1), (array([ 0.40329629, -0.99085697]), 1), (array([0.33826029, 0.08682359]), 1), (array([0.13131723, 0.90477883]), 1), (array([0.78433276, 0.95531496]), 1), (array([-0.68512118,  0.32896758]), -1), (array([-0.95845166,  0.70130455]), -1), (array([-0.05376457, -0.42150367]), -1), (array([0.20566899, 0.27994551]), 1), (array([ 0.04955063, -0.88381012]), -1), (array([-0.46991512,  0.08771095]), -1), (array([-0.98152041, -0.30138969]), -1), (array([0.4077043 , 0.51226894]), 1), (array([ 0.99744233, -0.39556814]), 1), (array([-0.92360277,  0.36401419]), -1), (array([-0.95079624,  0.771999  ]), -1), (array([0.84447429, 0.58260253]), 1), (array([ 0.97052497, -0.75314282]), 1), (array([-0.17419443,  0.55930534]), 1), (array([0.62538867, 0.11302967]), 1), (array([-0.24124706,  0.14970595]), -1), (array([-0.4594264 ,  0.51614698]), -1), (array([ 0.74572417, -0.49623519]), 1), (array([0.63729488, 0.19354368]), 1), (array([-0.45778818, -0.50977974]), -1), (array([0.93266743, 0.01350394]), 1), (array([-0.60105763, -0.21116161]), -1), (array([0.61163662, 0.19990738]), 1), (array([-0.9124521 , -0.13932253]), -1), (array([-0.53118233,  0.32387185]), -1), (array([-0.61481891, -0.6497688 ]), -1), (array([0.12564114, 0.31492364]), 1), (array([0.95826522, 0.17394167]), 1), (array([-0.06231653, -0.32809812]), -1), (array([ 0.01351287, -0.05963918]), 1), (array([ 0.640344  , -0.62690088]), 1), (array([-0.62375215, -0.37980406]), -1), (array([ 0.46644205, -0.4545791 ]), 1), (array([0.04326924, 0.1849239 ]), 1), (array([-0.44257246, -0.41231203]), -1), (array([-0.98592086, -0.88152293]), -1), (array([ 0.76744317, -0.19790956]), 1), (array([ 0.96105459, -0.26706013]), 1), (array([-0.56026973, -0.87884294]), -1), (array([ 0.01956131, -0.93200092]), -1), (array([ 0.70366944, -0.50418832]), 1), (array([-0.24590017, -0.45660292]), -1), (array([-0.31355265, -0.31247184]), -1), (array([0.80517832, 0.98530662]), 1), (array([-0.74790763,  0.00619934]), -1), (array([-0.13811178,  0.48698666]), 1), (array([-0.97448427, -0.35763775]), -1), (array([ 0.47933298, -0.90808238]), 1), (array([-0.78700272, -0.28851239]), -1), (array([-0.54563113, -0.54253169]), -1), (array([-0.769651  , -0.88238201]), -1), (array([-0.30514742, -0.86168882]), -1), (array([-0.22560869,  0.06603497]), -1), (array([0.81288316, 0.78125957]), 1), (array([-0.2888874,  0.1112777]), -1), (array([-0.94821044,  0.12502422]), -1), (array([-0.39610068, -0.42496345]), -1), (array([-0.79729741, -0.67078347]), -1), (array([-0.83833809, -0.92509453]), -1), (array([-0.67779976, -0.12544237]), -1), (array([-0.32778417,  0.95961751]), 1), (array([-0.22828666, -0.90063165]), -1), (array([-0.7783473 ,  0.23924637]), -1), (array([0.80626833, 0.06892471]), 1), (array([ 0.26644488, -0.21719677]), 1), (array([-0.38867245, -0.55800586]), -1), (array([0.77331568, 0.16012789]), 1), (array([0.07969528, 0.967621  ]), 1), (array([-0.52180153, -0.76782181]), -1), (array([-0.19763287, -0.02501517]), -1), (array([-0.4407147,  0.9065109]), -1), (array([-0.21446815, -0.5169435 ]), -1), (array([ 0.28763101, -0.26380516]), 1), (array([ 0.45153529, -0.8875703 ]), 1), (array([ 0.98132549, -0.69051699]), 1), (array([0.74558856, 0.08276119]), 1), (array([-0.45734448, -0.709066  ]), -1), (array([-0.85006925,  0.98190961]), -1), (array([-0.58726345, -0.69525499]), -1), (array([-0.69626399,  0.29310536]), -1), (array([-0.4283079 , -0.49097645]), -1), (array([ 0.96793243, -0.22884377]), 1), (array([-0.91386811,  0.16330914]), -1), (array([-0.9925936 ,  0.49809354]), -1), (array([-0.24830722, -0.50387169]), -1), (array([-0.84658206, -0.88575155]), -1), (array([ 0.18164524, -0.11653588]), 1), (array([-0.85967629,  0.55101047]), -1), (array([0.46790962, 0.07924893]), 1), (array([-0.51067047, -0.4188887 ]), -1), (array([ 0.11308745, -0.68401411]), -1), (array([0.8069856 , 0.17930501]), 1), (array([ 0.15988067, -0.16557893]), 1), (array([-0.78731873, -0.08434025]), -1)]\n",
            "[(array([0.08073677, 0.9464575 ]), -1), (array([-0.22359324,  0.3010598 ]), -1), (array([-0.54163453,  0.684263  ]), -1), (array([ 0.87492461, -0.7729639 ]), -1), (array([0.08064527, 0.96724759]), -1), (array([-0.83621104,  0.57689084]), -1), (array([-0.79081688, -0.76172776]), -1), (array([0.40656499, 0.06054627]), -1), (array([ 0.65074435, -0.68913839]), -1), (array([-0.89572975,  0.65412432]), -1), (array([0.88532867, 0.87594744]), -1), (array([0.47653549, 0.78484563]), -1), (array([-0.44459589,  0.5351472 ]), -1), (array([ 0.5074473 , -0.37654821]), -1), (array([0.32977094, 0.49825222]), -1), (array([0.81774974, 0.3497325 ]), -1), (array([ 0.10732239, -0.46056524]), -1), (array([-0.90705819,  0.37682392]), -1), (array([ 0.86495834, -0.67836164]), -1), (array([0.54833035, 0.70877745]), -1), (array([-0.38673002, -0.83774817]), -1), (array([-0.54712808,  0.05519486]), -1), (array([0.75070516, 0.38328938]), -1), (array([-0.1520814 , -0.74259983]), -1), (array([ 0.90198097, -0.13060515]), -1), (array([ 0.31523956, -0.41715614]), -1), (array([-0.93463417, -0.84534029]), -1), (array([-0.80719027, -0.30814257]), -1), (array([ 0.76508664, -0.44249843]), -1), (array([0.6176235 , 0.23880387]), -1), (array([0.61819166, 0.21541967]), -1), (array([-0.82116036,  0.38632803]), -1), (array([-0.49269212,  0.46397122]), -1), (array([ 0.9774988 , -0.30364667]), -1), (array([-0.35916188,  0.76260373]), -1), (array([-0.94337175,  0.71782326]), 1), (array([-0.92209337, -0.4964882 ]), -1), (array([-0.39537156,  0.40905167]), -1), (array([ 0.45743076, -0.82684408]), -1), (array([0.87988552, 0.73767356]), -1), (array([0.00744996, 0.61627623]), -1), (array([-0.48417694, -0.23996625]), -1), (array([ 0.9628511, -0.6259082]), -1), (array([0.60440352, 0.26556737]), -1), (array([-0.79977714, -0.31051783]), -1), (array([-0.27826987, -0.28122517]), -1), (array([-0.54008263,  0.1308429 ]), -1), (array([-0.33806917, -0.13160835]), -1), (array([0.0205644 , 0.26680592]), -1), (array([0.39704814, 0.45688682]), -1), (array([ 0.58552032, -0.06850171]), -1), (array([0.24474615, 0.91773743]), -1), (array([ 0.61996546, -0.62866362]), -1), (array([-0.53333015,  0.56349242]), -1), (array([-0.78779751, -0.57049512]), -1), (array([-0.80794685, -0.91473883]), -1), (array([-0.1181315 , -0.99125791]), -1), (array([-0.3791483 , -0.89633187]), -1), (array([0.65306684, 0.49674736]), -1), (array([0.05227239, 0.29451404]), -1), (array([ 0.39932175, -0.78940025]), -1), (array([-0.81513715, -0.77325142]), -1), (array([0.4671881 , 0.83967355]), -1), (array([-0.49472566, -0.6865719 ]), -1), (array([-0.46318407, -0.73779769]), -1), (array([0.76851911, 0.6246082 ]), -1), (array([0.56023155, 0.83991997]), -1), (array([-0.70427514,  0.27738243]), -1), (array([ 0.3512548 , -0.59344166]), -1), (array([-0.60122716, -0.0529193 ]), -1), (array([ 0.92614885, -0.20126778]), -1), (array([-0.14407087, -0.74512582]), -1), (array([0.67679388, 0.06805146]), -1), (array([-0.47856496,  0.39718682]), -1), (array([ 0.90829157, -0.08552764]), -1), (array([-0.75409518,  0.42730023]), -1), (array([0.69156075, 0.55317902]), -1), (array([-0.1657669 ,  0.77059991]), -1), (array([-0.76842462, -0.62645359]), -1), (array([-0.0521159 ,  0.82904787]), -1), (array([-0.38157046,  0.3040607 ]), -1), (array([0.34819083, 0.85603504]), -1), (array([-0.7890886 , -0.17664819]), -1), (array([-0.90074907,  0.20505684]), -1), (array([-0.43497636, -0.80962107]), -1), (array([0.45387558, 0.74780921]), -1), (array([ 0.18238396, -0.40130588]), -1), (array([ 0.13549279, -0.20477013]), -1), (array([-0.50299935, -0.74393777]), -1), (array([-0.5096407 ,  0.71036883]), -1), (array([-0.2339116 ,  0.12709655]), -1), (array([-0.42492197, -0.42422806]), -1), (array([-0.32871248,  0.52099728]), -1), (array([-0.64268577, -0.69416697]), -1), (array([-0.966701  , -0.20931175]), -1), (array([-0.43643316,  0.63480542]), -1), (array([-0.59821417, -0.23169702]), -1), (array([0.70634126, 0.25010257]), -1), (array([ 0.59466529, -0.07887979]), -1), (array([0.79132506, 0.26326785]), -1)]\n",
            "[(array([ 0.81684582, -0.29995122]), -1), (array([-0.24942364,  0.98001231]), -1), (array([ 0.8526197, -0.8545484]), -1), (array([ 0.87961282, -0.29671106]), -1), (array([ 0.55287554, -0.28495832]), -1), (array([0.79779725, 0.34083106]), 1), (array([ 0.03857304, -0.85848898]), -1), (array([-0.06375108,  0.28926036]), -1), (array([-0.42613134,  0.6175854 ]), -1), (array([ 0.68643778, -0.79281075]), -1), (array([-0.52380677, -0.70351372]), -1), (array([0.04833401, 0.44903694]), -1), (array([-0.41008843,  0.09644177]), -1), (array([0.8778056 , 0.83910106]), 1), (array([-0.51879495,  0.29274444]), -1), (array([-0.12145944,  0.30325215]), -1), (array([-0.44510367,  0.2027339 ]), -1), (array([-0.37524902,  0.93489785]), -1), (array([-0.54267635,  0.6311379 ]), -1), (array([ 0.84091421, -0.27326647]), -1), (array([ 0.3788923 , -0.78776896]), -1), (array([-0.73424246,  0.06624373]), -1), (array([-0.21881535,  0.71056602]), -1), (array([0.84006209, 0.13519623]), -1), (array([ 0.83404204, -0.74360657]), -1), (array([-0.30562748,  0.61208089]), -1), (array([0.75915982, 0.4343526 ]), 1), (array([-0.13637845,  0.40426881]), -1), (array([-0.35423671, -0.06295251]), -1), (array([0.70940478, 0.28810857]), -1), (array([0.88914859, 0.63397453]), 1), (array([0.20137063, 0.43761727]), -1), (array([ 0.8547317 , -0.34590767]), -1), (array([-0.95563465, -0.51050661]), -1), (array([ 0.82504599, -0.4059417 ]), -1), (array([0.69797624, 0.49444559]), -1), (array([-0.55782418, -0.71020321]), -1), (array([-0.69609755,  0.0799411 ]), -1), (array([0.96232598, 0.66924966]), 1), (array([0.43946553, 0.49264513]), -1), (array([-0.11698169,  0.68241758]), -1), (array([ 0.57675779, -0.03489439]), -1), (array([-0.19546433, -0.70029061]), -1), (array([0.44802453, 0.78566468]), -1), (array([-0.90650485, -0.89088839]), -1), (array([-0.42397649,  0.03904946]), -1), (array([-0.0758447 ,  0.17982677]), -1), (array([0.46534019, 0.5957736 ]), -1), (array([0.78611018, 0.66241327]), 1), (array([-0.50041073, -0.02348635]), -1), (array([ 0.59334073, -0.55124946]), -1), (array([-0.7192471, -0.3395331]), -1), (array([ 0.96342324, -0.38975478]), -1), (array([0.14969345, 0.13489099]), -1), (array([-0.20497066,  0.15364744]), -1), (array([ 0.07160118, -0.1133305 ]), -1), (array([ 0.51518804, -0.4317644 ]), -1), (array([0.63539744, 0.72065057]), -1), (array([-0.4589583 , -0.88121592]), -1), (array([0.21940251, 0.27155978]), -1), (array([ 0.39342823, -0.11512666]), -1), (array([ 0.10053777, -0.60555176]), -1), (array([-0.1958889, -0.3444217]), -1), (array([ 0.73924528, -0.22132762]), -1), (array([-0.01951034,  0.99219325]), -1), (array([ 0.82785429, -0.50389274]), -1), (array([0.03645439, 0.10738719]), -1), (array([0.31147488, 0.8124441 ]), -1), (array([ 0.50341326, -0.80902386]), -1), (array([0.47652445, 0.48556893]), -1), (array([-0.81274667, -0.87163974]), -1), (array([0.49970095, 0.90817985]), -1), (array([-0.71712261, -0.82025545]), -1), (array([ 0.71547494, -0.03177103]), -1), (array([-0.61042628, -0.93720444]), -1), (array([0.43288985, 0.59931798]), -1), (array([-0.15764932, -0.3342132 ]), -1), (array([-0.1644252 , -0.41672433]), -1), (array([0.02077508, 0.46815662]), -1), (array([0.46472322, 0.51856122]), -1), (array([-0.68460432, -0.97364393]), -1), (array([ 0.91598473, -0.19471837]), -1), (array([-0.14935392,  0.5936979 ]), -1), (array([0.32533599, 0.03545649]), -1), (array([0.45620321, 0.26375788]), -1), (array([0.95753932, 0.63254914]), 1), (array([0.04100983, 0.90655434]), -1), (array([-0.78013992, -0.26172991]), -1), (array([ 0.57062168, -0.27496763]), -1), (array([-0.52154801,  0.88871118]), -1), (array([-0.79132672, -0.4140888 ]), -1), (array([0.15056724, 0.34734642]), -1), (array([0.11232871, 0.03966057]), -1), (array([0.09518088, 0.2455832 ]), -1), (array([-0.05878388,  0.09256837]), -1), (array([0.91013092, 0.14038381]), 1), (array([ 0.66186924, -0.27216434]), -1), (array([0.17948564, 0.13586267]), -1), (array([ 0.95835468, -0.06943725]), 1), (array([-0.71168895,  0.61254392]), -1)]\n",
            "[(array([0.45763964, 0.36476237]), 1), (array([ 0.62541167, -0.67690214]), -1), (array([-0.05058309, -0.37212178]), 1), (array([ 0.7731756 , -0.15877089]), 1), (array([0.84606399, 0.40781585]), 1), (array([0.60882853, 0.88784645]), 1), (array([ 0.77943431, -0.84899982]), -1), (array([-0.2011121 , -0.51081536]), -1), (array([ 0.80265375, -0.97538474]), -1), (array([0.89594153, 0.36525623]), 1), (array([0.78774259, 0.53360065]), 1), (array([-0.84365298,  0.0576008 ]), 1), (array([0.94151531, 0.70138424]), 1), (array([0.82162323, 0.5718484 ]), 1), (array([ 0.71840288, -0.90085074]), -1), (array([-0.33596122, -0.13769076]), 1), (array([-0.94437257,  0.36041226]), 1), (array([ 0.28660936, -0.80678864]), -1), (array([0.36724176, 0.75785171]), 1), (array([0.57240555, 0.75761922]), 1), (array([ 0.27063141, -0.857206  ]), -1), (array([ 0.56653881, -0.1269183 ]), 1), (array([ 0.11645638, -0.92414758]), -1), (array([ 0.59915131, -0.63312887]), -1), (array([0.03766965, 0.61343122]), 1), (array([-0.41438519,  0.94074158]), 1), (array([0.6344739 , 0.81249268]), 1), (array([ 0.58070965, -0.82026833]), -1), (array([0.32666813, 0.87372814]), 1), (array([-0.51106114,  0.36235046]), 1), (array([ 0.56886524, -0.45617833]), 1), (array([ 0.98662759, -0.40918691]), 1), (array([-0.85207497, -0.36565524]), -1), (array([0.06964771, 0.01573334]), 1), (array([0.04095188, 0.31437947]), 1), (array([ 0.81896126, -0.78596091]), -1), (array([-0.90506858,  0.51169294]), 1), (array([0.21910257, 0.28352306]), 1), (array([0.75028394, 0.88089788]), 1), (array([-0.3765056 ,  0.95752144]), 1), (array([ 0.03820468, -0.85214409]), -1), (array([0.58645964, 0.77868004]), 1), (array([ 0.65486528, -0.19216946]), 1), (array([ 0.91967053, -0.02856646]), 1), (array([0.28291485, 0.20118577]), 1), (array([-0.22784957,  0.10333412]), 1), (array([0.81455862, 0.77725607]), 1), (array([ 0.56576223, -0.22596037]), 1), (array([-0.96390979, -0.19666735]), 1), (array([0.39319681, 0.33567113]), 1), (array([0.68138605, 0.04897274]), 1), (array([-0.83526875,  0.07673579]), 1), (array([ 0.03340456, -0.3362253 ]), 1), (array([0.12434614, 0.57481878]), 1), (array([-0.52872064, -0.42948824]), -1), (array([-0.64368638, -0.19238898]), 1), (array([-0.45858896,  0.25947276]), 1), (array([ 0.85087307, -0.5405137 ]), 1), (array([ 0.14427088, -0.09862418]), 1), (array([ 0.08402746, -0.00450678]), 1), (array([0.6237321, 0.4578827]), 1), (array([-0.49468394, -0.03280233]), 1), (array([ 0.4441097 , -0.43761996]), 1), (array([0.84392406, 0.20896028]), 1), (array([-0.30748071,  0.04750297]), 1), (array([-0.99289172,  0.41107998]), 1), (array([ 0.48096928, -0.99593358]), -1), (array([ 0.63713636, -0.2423866 ]), 1), (array([0.08825045, 0.17505859]), 1), (array([0.52163001, 0.9829413 ]), 1), (array([-0.98523054,  0.21372358]), 1), (array([ 0.2407764 , -0.88636165]), -1), (array([-0.52892517,  0.9259969 ]), 1), (array([-0.89578856,  0.89210418]), 1), (array([-0.80717135, -0.94725705]), -1), (array([-0.35034686, -0.91813332]), -1), (array([-0.63042959, -0.28078831]), 1), (array([-0.40910066, -0.64000283]), -1), (array([-0.50141867,  0.89690633]), 1), (array([ 0.37702986, -0.17066955]), 1), (array([0.36525022, 0.17152843]), 1), (array([-0.78036409,  0.35742441]), 1), (array([ 0.05139855, -0.63769231]), -1), (array([-0.66358112, -0.07598604]), 1), (array([-0.73103443, -0.97232545]), -1), (array([0.49799616, 0.23487156]), 1), (array([0.29750575, 0.00681064]), 1), (array([ 0.54690336, -0.77391842]), -1), (array([-0.005714  , -0.77180581]), -1), (array([-0.97623057, -0.92262566]), -1), (array([-0.60915294,  0.99572624]), 1), (array([-0.97450555, -0.17854796]), 1), (array([0.96730374, 0.46304288]), 1), (array([0.53784076, 0.92941309]), 1), (array([-0.92324703, -0.85174548]), -1), (array([ 0.17534296, -0.41287819]), 1), (array([0.39357063, 0.65984021]), 1), (array([-0.73073859, -0.22438249]), 1), (array([0.05139814, 0.03752212]), 1), (array([-0.61582795, -0.00975286]), 1)]\n",
            "[(array([-0.93204638,  0.26075827]), -1), (array([-0.55827195,  0.1537707 ]), 1), (array([-0.48081869,  0.36869407]), 1), (array([0.88019661, 0.67960455]), 1), (array([ 0.63977334, -0.86348521]), 1), (array([-0.26260427,  0.62541243]), 1), (array([ 0.10081617, -0.26785942]), 1), (array([ 0.75540119, -0.99855363]), 1), (array([-0.34252675,  0.20378207]), 1), (array([-0.63385755, -0.33685009]), -1), (array([-0.19996881,  0.46509736]), 1), (array([0.15887479, 0.6097989 ]), 1), (array([ 0.03559466, -0.81042587]), -1), (array([0.27241765, 0.20349866]), 1), (array([-0.60260163, -0.37311385]), -1), (array([ 0.43750589, -0.03949345]), 1), (array([ 0.20468472, -0.41429787]), 1), (array([-0.94603059,  0.51308595]), -1), (array([0.18510028, 0.63303514]), 1), (array([-0.23216273,  0.47137031]), 1), (array([-0.71132197,  0.8854315 ]), 1), (array([0.66454529, 0.22085509]), 1), (array([ 0.96694616, -0.44323291]), 1), (array([-0.94589816,  0.2265471 ]), -1), (array([0.15214551, 0.01747874]), 1), (array([-0.08727264,  0.87785828]), 1), (array([0.98078276, 0.08305173]), 1), (array([-0.33651772,  0.09715482]), 1), (array([0.31768321, 0.91917537]), 1), (array([-0.71524529,  0.32826632]), 1), (array([0.81265647, 0.47317917]), 1), (array([-0.81962741,  0.14186686]), -1), (array([ 0.06623339, -0.14405203]), 1), (array([-0.19115865,  0.39781009]), 1), (array([0.0202275 , 0.82252122]), 1), (array([-0.72922745, -0.82089513]), -1), (array([0.48102047, 0.88717876]), 1), (array([-0.41528702, -0.32531275]), -1), (array([-0.26362671, -0.91364249]), -1), (array([-0.96791581, -0.07058647]), -1), (array([-0.53405185, -0.30480613]), -1), (array([ 0.046756  , -0.06314433]), 1), (array([ 0.80718567, -0.8350721 ]), 1), (array([ 0.99633513, -0.73300572]), 1), (array([ 0.6003108 , -0.21488815]), 1), (array([-0.52796443,  0.32519128]), 1), (array([-0.06613071, -0.23051354]), 1), (array([ 0.6038997 , -0.77445994]), 1), (array([0.65632388, 0.16378753]), 1), (array([0.05489683, 0.52090755]), 1), (array([0.95034496, 0.22468559]), 1), (array([-0.68285049, -0.15900769]), -1), (array([ 0.74810185, -0.32767951]), 1), (array([ 0.71875233, -0.45744981]), 1), (array([ 0.49200883, -0.52717355]), 1), (array([-0.58570167, -0.76392322]), -1), (array([-0.40876184,  0.61102815]), 1), (array([ 0.13163371, -0.9220583 ]), -1), (array([-0.83771586, -0.19476114]), -1), (array([-0.86960353, -0.2523109 ]), -1), (array([0.33280877, 0.35507413]), 1), (array([ 0.48102135, -0.66683025]), 1), (array([0.96258034, 0.0149446 ]), 1), (array([-0.38377811,  0.31656793]), 1), (array([-0.30204462, -0.79030035]), -1), (array([ 0.43447772, -0.55060538]), 1), (array([ 0.08937208, -0.83221553]), -1), (array([-0.64992514,  0.70514733]), 1), (array([-0.30727764,  0.93562725]), 1), (array([-0.90206314, -0.39622319]), -1), (array([0.61619344, 0.25263128]), 1), (array([0.85063383, 0.27170004]), 1), (array([0.52352107, 0.90658257]), 1), (array([-0.25052802, -0.6054346 ]), -1), (array([-0.15185691, -0.64814267]), -1), (array([ 0.38773503, -0.09322393]), 1), (array([-0.10919472, -0.77826259]), -1), (array([-0.71027421, -0.87388079]), -1), (array([0.2677301 , 0.27270483]), 1), (array([ 0.08547656, -0.60472665]), 1), (array([-0.92803567,  0.49947957]), -1), (array([ 0.36428369, -0.11664128]), 1), (array([-0.17899383,  0.06737122]), 1), (array([-0.85635054,  0.06462797]), -1), (array([0.29009602, 0.62534088]), 1), (array([-0.61675847,  0.60071098]), 1), (array([0.93391492, 0.40677571]), 1), (array([-0.00852072,  0.65281955]), 1), (array([0.2880027 , 0.58049202]), 1), (array([-0.7399103 ,  0.58466664]), 1), (array([-0.32643502, -0.59320585]), -1), (array([0.11201028, 0.78249206]), 1), (array([-0.28986929,  0.85819694]), 1), (array([ 0.27462437, -0.08606983]), 1), (array([-0.05882359,  0.16953367]), 1), (array([0.63755208, 0.46512292]), 1), (array([ 0.93206229, -0.22552057]), 1), (array([-0.79481381, -0.35781511]), -1), (array([-0.25832275, -0.74921506]), -1), (array([0.39712267, 0.88809866]), 1)]\n",
            "[(array([-0.66039108, -0.62572989]), 1), (array([-0.05527645, -0.5001404 ]), -1), (array([ 0.0498727 , -0.18408792]), -1), (array([-0.49785546,  0.67960431]), 1), (array([-0.58413328,  0.24381551]), 1), (array([-0.99455901, -0.83137492]), 1), (array([0.74041095, 0.35288978]), -1), (array([ 0.28345603, -0.52723251]), -1), (array([-0.24439723, -0.51175901]), 1), (array([0.06690113, 0.00158218]), 1), (array([-0.79098584,  0.56087948]), 1), (array([-0.62348892, -0.54494866]), 1), (array([ 0.43806623, -0.41307028]), -1), (array([-0.66817924,  0.84189246]), 1), (array([-0.73342046,  0.83143671]), 1), (array([-0.62044441, -0.2460253 ]), 1), (array([-0.26658134,  0.20845719]), 1), (array([-0.77989225,  0.75605013]), 1), (array([-0.90710748,  0.72538627]), 1), (array([ 0.50524354, -0.5565588 ]), -1), (array([-0.90889172,  0.09583716]), 1), (array([-0.76321785, -0.31896618]), 1), (array([0.33248134, 0.78640545]), 1), (array([0.45252048, 0.94361628]), 1), (array([ 0.44646214, -0.87142078]), -1), (array([-0.9809501,  0.4504609]), 1), (array([ 0.91603236, -0.93940575]), -1), (array([-0.18549357, -0.8166882 ]), -1), (array([ 0.06937958, -0.73222949]), -1), (array([0.01706786, 0.75659503]), 1), (array([-0.20941018, -0.36129233]), 1), (array([-0.06214045,  0.57068806]), 1), (array([-0.23249261, -0.29514967]), 1), (array([0.61289289, 0.40199827]), -1), (array([-0.08323779,  0.28971507]), 1), (array([ 0.5693761 , -0.10689694]), -1), (array([ 0.82916429, -0.43130949]), -1), (array([-0.98473064, -0.50303123]), 1), (array([-0.62016837, -0.71799188]), 1), (array([0.31692592, 0.26320893]), -1), (array([-0.88448824, -0.88216877]), 1), (array([ 0.61020613, -0.97476124]), -1), (array([0.29094268, 0.17823094]), -1), (array([0.19532883, 0.40190079]), 1), (array([0.12109462, 0.00766566]), -1), (array([ 0.77637655, -0.98234766]), -1), (array([ 0.61681602, -0.54281193]), -1), (array([0.17350314, 0.10950489]), -1), (array([ 0.27793804, -0.6033051 ]), -1), (array([-0.42357448, -0.86930905]), 1), (array([-0.66229981,  0.77784219]), 1), (array([0.93861632, 0.96391587]), -1), (array([0.51556153, 0.79475936]), 1), (array([-0.86022384, -0.27356189]), 1), (array([-0.62752269, -0.90151551]), 1), (array([-0.18510178,  0.92807179]), 1), (array([-0.10076101,  0.71891245]), 1), (array([-0.82873738,  0.51821385]), 1), (array([0.4497096 , 0.04765777]), -1), (array([-0.05505314,  0.22311742]), 1), (array([-0.42109655, -0.6916708 ]), 1), (array([-0.45703467,  0.66187248]), 1), (array([0.65520499, 0.35746181]), -1), (array([0.59854864, 0.32333105]), -1), (array([-0.63524631, -0.19984091]), 1), (array([-0.85555206,  0.53898122]), 1), (array([ 0.54761533, -0.02002454]), -1), (array([0.01566497, 0.11129148]), 1), (array([-0.1379849 ,  0.75644698]), 1), (array([ 0.33862462, -0.71676872]), -1), (array([-0.35645071,  0.58483578]), 1), (array([0.47023435, 0.20753169]), -1), (array([0.49327383, 0.00859657]), -1), (array([-0.02183764,  0.85082197]), 1), (array([0.24296662, 0.33313241]), 1), (array([-0.32852127,  0.05788237]), 1), (array([-0.51988423, -0.84519206]), 1), (array([-0.89846324,  0.92704751]), 1), (array([-0.4803871 , -0.67422106]), 1), (array([0.67434258, 0.07094422]), -1), (array([ 0.94566393, -0.57371039]), -1), (array([ 0.59099803, -0.84883072]), -1), (array([-0.03625273,  0.80279702]), 1), (array([0.03121026, 0.38060211]), 1), (array([ 0.35918062, -0.63189935]), -1), (array([ 0.79384294, -0.88043973]), -1), (array([-0.24592814,  0.99269702]), 1), (array([-0.40038996,  0.3316693 ]), 1), (array([0.89589923, 0.48659008]), -1), (array([-0.09479313, -0.6327565 ]), -1), (array([ 0.91090321, -0.91118838]), -1), (array([0.0400969 , 0.57216775]), 1), (array([0.57449372, 0.3600955 ]), -1), (array([-0.53612375,  0.72728629]), 1), (array([ 0.36686612, -0.80324178]), -1), (array([0.65635463, 0.57072957]), -1), (array([ 0.42684888, -0.5262891 ]), -1), (array([-0.22906087, -0.03494272]), 1), (array([-0.11592319, -0.98315287]), -1), (array([0.7191419 , 0.42966213]), -1)]\n",
            "[(array([ 0.93692964, -0.85170919]), 1), (array([-0.22327384,  0.48375809]), 1), (array([-0.15909021,  0.98241511]), 1), (array([-0.94872579,  0.70258732]), -1), (array([-0.25212318,  0.82676201]), 1), (array([0.94382073, 0.34228692]), 1), (array([-0.69494296,  0.35110246]), -1), (array([0.63519365, 0.21920534]), 1), (array([ 0.22962741, -0.66916275]), 1), (array([0.99912784, 0.67029446]), 1), (array([-0.48604562, -0.53948861]), -1), (array([-0.21548722, -0.3029029 ]), 1), (array([0.95639416, 0.73989701]), 1), (array([-0.10456424,  0.09423683]), 1), (array([-0.44962132,  0.7057819 ]), 1), (array([ 0.05593963, -0.25728645]), 1), (array([0.71601621, 0.09492218]), 1), (array([ 0.05772727, -0.85102105]), -1), (array([ 0.89035989, -0.64386838]), 1), (array([ 0.84297901, -0.47860903]), 1), (array([0.52363368, 0.24782198]), 1), (array([-0.10233627,  0.2934235 ]), 1), (array([-0.19517289,  0.45777687]), 1), (array([-0.99216041, -0.09657026]), -1), (array([-0.99532142, -0.29919742]), -1), (array([0.24556022, 0.2282305 ]), 1), (array([0.15002559, 0.55660848]), 1), (array([ 0.04039506, -0.32207935]), 1), (array([-0.72961704, -0.3579159 ]), -1), (array([-0.39141541,  0.6256809 ]), 1), (array([ 0.93567099, -0.59283808]), 1), (array([-0.81173739, -0.52444553]), -1), (array([-0.00160097,  0.46241169]), 1), (array([-0.93740189,  0.77427791]), 1), (array([ 0.17449479, -0.80834623]), 1), (array([ 0.94847263, -0.27728194]), 1), (array([0.75713811, 0.05629862]), 1), (array([-0.00415274, -0.24923238]), 1), (array([-0.7574053 , -0.86666435]), -1), (array([ 0.53917868, -0.462461  ]), 1), (array([ 0.89829006, -0.93259401]), 1), (array([-0.92081184, -0.38485658]), -1), (array([0.24782728, 0.03403923]), 1), (array([ 0.53737689, -0.62489166]), 1), (array([-0.60111995, -0.71659068]), -1), (array([ 0.34667779, -0.90726267]), 1), (array([-0.31199608, -0.53715104]), -1), (array([-0.40598308,  0.72157304]), 1), (array([-0.15706299,  0.55783643]), 1), (array([-0.47691689,  0.9064458 ]), 1), (array([ 0.86933901, -0.83641397]), 1), (array([-0.37395958, -0.99805002]), -1), (array([0.29689387, 0.63263386]), 1), (array([0.25521525, 0.19841059]), 1), (array([-0.2227519 ,  0.10110929]), 1), (array([-0.78836007, -0.00737921]), -1), (array([-0.31950217,  0.66648022]), 1), (array([-0.16841814, -0.6412335 ]), -1), (array([ 0.79263511, -0.74294018]), 1), (array([-0.71129742,  0.13925256]), -1), (array([-0.49864755,  0.43980272]), 1), (array([-0.76032982,  0.46093344]), 1), (array([0.7112836 , 0.32237007]), 1), (array([-0.62577025, -0.14194064]), -1), (array([0.40741284, 0.82513214]), 1), (array([-0.30626636, -0.56591048]), -1), (array([0.95213569, 0.27442922]), 1), (array([-0.72128987, -0.26610644]), -1), (array([ 0.22121725, -0.62321466]), 1), (array([0.67630357, 0.3421595 ]), 1), (array([ 0.92199263, -0.18081511]), 1), (array([ 0.86619617, -0.635521  ]), 1), (array([-0.35573864, -0.54467787]), -1), (array([-0.85071802,  0.09860342]), -1), (array([ 0.24347293, -0.7510848 ]), 1), (array([ 0.04982922, -0.12315624]), 1), (array([-0.39534591,  0.24345636]), 1), (array([-0.02580388,  0.28749149]), 1), (array([-0.45455055, -0.76693078]), -1), (array([-0.8690147 , -0.59265741]), -1), (array([-0.79958768,  0.78568559]), 1), (array([ 0.38997523, -0.3732463 ]), 1), (array([-0.63128438,  0.22495453]), -1), (array([-0.6204484 , -0.66718208]), -1), (array([-0.12037532, -0.29798688]), 1), (array([ 0.57455909, -0.48838994]), 1), (array([-0.09589705, -0.15561132]), 1), (array([0.89493002, 0.7891062 ]), 1), (array([-0.58485303, -0.07139528]), -1), (array([-0.17695499,  0.90117007]), 1), (array([-0.26470282,  0.11460246]), 1), (array([-0.83994872, -0.36307895]), -1), (array([-0.99802153, -0.34499941]), -1), (array([ 0.55707214, -0.44397177]), 1), (array([0.35474432, 0.24065016]), 1), (array([-0.3331567 , -0.94950262]), -1), (array([-0.37395475, -0.72210827]), -1), (array([-0.9478982 , -0.92433959]), -1), (array([-0.43924901,  0.73757983]), 1), (array([0.59905267, 0.27956103]), 1)]\n",
            "[(array([-0.7880275,  0.1218312]), -1), (array([-0.04680102,  0.01883331]), -1), (array([ 0.42157705, -0.99128439]), -1), (array([0.07514002, 0.40134182]), 1), (array([-0.36517914, -0.48595526]), -1), (array([-0.03690706, -0.55572792]), -1), (array([ 0.74217083, -0.9140299 ]), -1), (array([0.44541732, 0.46321189]), 1), (array([-0.86149568,  0.47353598]), 1), (array([0.65888751, 0.30892204]), 1), (array([-0.78631397, -0.80843139]), -1), (array([-0.06749392, -0.6343364 ]), -1), (array([-0.86524857, -0.58570752]), -1), (array([-0.23567849,  0.2084952 ]), 1), (array([ 0.59624843, -0.49217576]), -1), (array([-0.98708827,  0.80733699]), 1), (array([0.12781368, 0.36975861]), 1), (array([-0.20552987,  0.45309333]), 1), (array([-0.67994226, -0.57826325]), -1), (array([-0.62906877,  0.31665353]), 1), (array([-0.7014198 , -0.57925032]), -1), (array([-0.19238168, -0.51302027]), -1), (array([-0.84795627, -0.76902427]), -1), (array([0.8629713 , 0.50411529]), 1), (array([-0.59495564, -0.51418741]), -1), (array([0.97904421, 0.15292993]), 1), (array([0.66715068, 0.9182243 ]), 1), (array([0.19366355, 0.13692962]), 1), (array([ 0.29396021, -0.03392962]), -1), (array([0.09875189, 0.12338331]), 1), (array([0.53310509, 0.95793453]), 1), (array([0.40672403, 0.48727119]), 1), (array([-0.1609838 , -0.50234416]), -1), (array([-0.64600496, -0.02317887]), -1), (array([-0.27620267, -0.86124807]), -1), (array([-0.29442669,  0.6863823 ]), 1), (array([0.99511127, 0.33486692]), 1), (array([-0.11586583, -0.25765833]), -1), (array([ 0.28487525, -0.42584508]), -1), (array([-0.13945648,  0.15695908]), 1), (array([-0.43764689,  0.59501096]), 1), (array([0.28220471, 0.80939558]), 1), (array([-0.81978138, -0.651945  ]), -1), (array([ 0.9562752 , -0.94145506]), -1), (array([-0.66693359,  0.06794331]), -1), (array([ 0.28725642, -0.60749416]), -1), (array([-0.04266511, -0.53530754]), -1), (array([-0.20372406,  0.07698215]), -1), (array([-0.57288166, -0.19826356]), -1), (array([ 0.60914746, -0.92969534]), -1), (array([-0.91517879, -0.10011223]), -1), (array([-0.54532845, -0.86724965]), -1), (array([-0.01784959, -0.45402366]), -1), (array([-0.78224745,  0.85417178]), 1), (array([ 0.56090968, -0.38563668]), -1), (array([ 0.85819616, -0.41356812]), -1), (array([0.09883287, 0.48315579]), 1), (array([ 0.54646574, -0.78077194]), -1), (array([-0.7108026, -0.3229948]), -1), (array([ 0.0415907 , -0.23806642]), -1), (array([-0.56143105, -0.74629462]), -1), (array([ 0.13614285, -0.75558134]), -1), (array([-0.23227355,  0.36577936]), 1), (array([-0.31249693,  0.67256778]), 1), (array([-0.24724043, -0.7683945 ]), -1), (array([ 0.29250788, -0.78411729]), -1), (array([-0.22995661, -0.19631952]), -1), (array([ 0.90169289, -0.05628751]), 1), (array([-0.59346469, -0.0320966 ]), -1), (array([ 0.82683123, -0.17809191]), 1), (array([ 0.28885451, -0.52973249]), -1), (array([ 0.89166943, -0.11395455]), 1), (array([0.16420956, 0.85814867]), 1), (array([-0.21132937, -0.16557279]), -1), (array([-0.47424986, -0.293898  ]), -1), (array([0.44533509, 0.96633468]), 1), (array([ 0.85962792, -0.33611726]), -1), (array([-0.87778618,  0.65805721]), 1), (array([-0.47183732,  0.15695956]), -1), (array([0.33545394, 0.19159613]), 1), (array([0.69307803, 0.045302  ]), 1), (array([-0.77114015, -0.48293486]), -1), (array([-0.36280787, -0.75521948]), -1), (array([-0.46436925, -0.48716012]), -1), (array([0.71235844, 0.57337288]), 1), (array([0.6777262 , 0.22353414]), 1), (array([ 0.81736182, -0.36537609]), -1), (array([-0.81728491, -0.97987607]), -1), (array([ 0.01640835, -0.1802346 ]), -1), (array([ 0.88744035, -0.00166312]), 1), (array([0.08970053, 0.6036627 ]), 1), (array([0.9787614 , 0.58600972]), 1), (array([-0.42719622, -0.78211419]), -1), (array([-0.06888913,  0.73497078]), 1), (array([-0.21173755, -0.32195445]), -1), (array([-0.49436255, -0.50194759]), -1), (array([ 0.42581971, -0.83735056]), -1), (array([ 0.47753638, -0.02431946]), 1), (array([-0.61978931,  0.85235821]), 1), (array([0.64785734, 0.4649721 ]), 1)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-521d808adf85>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mout_sample_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mE_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_E_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_sample_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-521d808adf85>\u001b[0m in \u001b[0;36msgd\u001b[0;34m(data, epsilon, nu)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-521d808adf85>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(s, y, w)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mx_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#include the bias term\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mnew_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_n\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#denominator of gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import sched_getscheduler\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "space = [-1, 1]\n",
        "N = 100 #number of training points\n",
        "iterations = 100\n",
        "nu = 0.01 #learning rate\n",
        "test_size = 10000\n",
        "num_epochs = 100\n",
        "\n",
        "\n",
        "def generate_line():\n",
        "  p1 = np.random.uniform(*space, 2)\n",
        "  p2 = np.random.uniform(*space, 2)\n",
        "  slope = (p2[1] - p1[1]) / (p2[0] - p1[0]) if p2[0] != p1[0] else np.inf\n",
        "  b = p1[1] - slope * p1[0]\n",
        "  return lambda x: slope * x + b\n",
        "\n",
        "def prepare_data(N, target_function):\n",
        "  def target(point):\n",
        "      return target_function(point[0])\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "  for i in range(N):\n",
        "    new_pt = np.random.uniform(*space, 2)  #generate random points\n",
        "    y = target(new_pt)\n",
        "    difference = y - new_pt[1]\n",
        "\n",
        "    X.append(new_pt)\n",
        "    Y.append(difference)\n",
        "\n",
        "  targets = np.sign(Y)  #convert differences to -1 or 1\n",
        "  return X, targets\n",
        "\n",
        "def sgd(X, y, nu, num_epochs):\n",
        "  model = SGDClassifier(loss = \"log_loss\", learning_rate=\"constant\", eta0=nu, max_iter=1, warm_start=True)\n",
        "  for epoch in range(num_epochs):\n",
        "    model.partial_fit(X, y, classes=np.array([-1, 1]))\n",
        "  return model\n",
        "\n",
        "def calc_e_out(model, test_size, target_function):\n",
        "  X_test, y_test = prepare_data(test_size, target_function)\n",
        "  y_prob = model.predict_proba(X_test)\n",
        "  return log_loss(y_test, y_prob)\n",
        "\n",
        "\n",
        "e_out_total = 0\n",
        "target_function = generate_line()\n",
        "for run in range(iterations):\n",
        "  X_train, y_train = prepare_data(N, target_function)\n",
        "  model = sgd(X_train, y_train, nu, num_epochs)\n",
        "  e_out = calc_e_out(model, test_size, target_function)\n",
        "  e_out_total += e_out\n",
        "\n",
        "avg_e_out = e_out_total / iterations\n",
        "\n",
        "print(f'Average E_out: {avg_e_out}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5IVW8BWGfJl",
        "outputId": "ce4fe2bc-162e-4ef4-8b6d-ec5412fd53be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average E_out: 0.16323845994612346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the space for random sampling\n",
        "space = [-1, 1]\n",
        "\n",
        "# Function to define a random boundary line\n",
        "def generate_line():\n",
        "    p1 = np.random.uniform(*space, 2)\n",
        "    p2 = np.random.uniform(*space, 2)\n",
        "    slope = (p2[1] - p1[1]) / (p2[0] - p1[0]) if p2[0] != p1[0] else np.inf\n",
        "    b = p1[1] - slope * p1[0]\n",
        "    return lambda x: slope * x + b\n",
        "\n",
        "# Prepare training data based on the generated line\n",
        "def prepare_data(N, target_function):\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(N):\n",
        "        new_pt = np.random.uniform(*space, 2)\n",
        "        y = target_function(new_pt[0])\n",
        "        difference = y - new_pt[1]\n",
        "        X.append(new_pt)\n",
        "        Y.append(difference)\n",
        "\n",
        "    targets = np.sign(Y)\n",
        "    return np.array(X), targets\n",
        "\n",
        "# Logistic function\n",
        "def logistic_func(z):\n",
        "    \"\"\"Logistic function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "#train logistic regression using stochastic gradient descent\n",
        "def train_logistic_regression(X, y, learning_rate=0.01, tol=0.01, max_iter=1000):\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    N = len(y)\n",
        "\n",
        "    for epoch in range(max_iter):\n",
        "        weights_prev = weights.copy()\n",
        "        #random permutation of indices for SGD\n",
        "        indices = np.random.permutation(N)\n",
        "        for i in indices:\n",
        "            z = np.dot(X[i], weights)\n",
        "            prediction = logistic_func(z)\n",
        "            error = y[i] - prediction\n",
        "            weights += learning_rate * error * X[i]  #update weights\n",
        "\n",
        "        #check for convergence\n",
        "        if np.linalg.norm(weights - weights_prev) < tol:\n",
        "            break\n",
        "\n",
        "    return weights\n",
        "\n",
        "#calculate cross-entropy loss\n",
        "def cross_entropy_loss(X, y, weights):\n",
        "    predictions = logistic_func(np.dot(X, weights))\n",
        "    return -np.mean(y * np.log(predictions + 1e-15) + (1 - y) * np.log(1 - predictions + 1e-15))\n",
        "\n",
        "# Evaluate model performance on a separate test dataset\n",
        "def evaluate_model(weights, num_test_points=1000, target_function=None):\n",
        "    X_test = np.random.uniform(*space, (num_test_points, 2))\n",
        "    #generate test labels based on the target function\n",
        "    Y_test = np.sign(target_function(X_test[:, 0]) - X_test[:, 1])\n",
        "    predictions = np.where(logistic_func(np.dot(X_test, weights)) >= 0.5, 1, -1)\n",
        "    return np.mean(predictions != Y_test)  #error rate\n",
        "\n",
        "\n",
        "N = 100  #number of training points\n",
        "num_runs = 100  #number of experiments\n",
        "errors = []\n",
        "\n",
        "target_function = generate_line()\n",
        "for _ in range(num_runs):\n",
        "    X, Y = prepare_data(N, target_function)\n",
        "    weights = train_logistic_regression(X, Y)  #train the model\n",
        "    E_out = evaluate_model(weights, target_function=target_function)  #evaluate the model\n",
        "    errors.append(E_out)\n",
        "\n",
        "average_error = np.mean(errors)\n",
        "print(f\"Average E_out over {num_runs} runs: {average_error:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fv-i-m_J0m5D",
        "outputId": "893c1e31-6630-4e4c-e4c0-0a3b1d9f8303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average E_out over 100 runs: 0.152280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the space for random sampling\n",
        "space = [-1, 1]\n",
        "\n",
        "def generate_line():\n",
        "    \"\"\"Generate a line based on two random points.\"\"\"\n",
        "    p1 = np.random.uniform(*space, 2)\n",
        "    p2 = np.random.uniform(*space, 2)\n",
        "    slope = (p2[1] - p1[1]) / (p2[0] - p1[0]) if p2[0] != p1[0] else np.inf\n",
        "    b = p1[1] - slope * p1[0]\n",
        "    return lambda x: slope * x + b\n",
        "\n",
        "def prepare_data(N, target_function):\n",
        "    \"\"\"Prepare random data points and their corresponding labels.\"\"\"\n",
        "    X = []\n",
        "    Y = []\n",
        "    for _ in range(N):\n",
        "        new_pt = np.random.uniform(*space, 2)  # Generate random points\n",
        "        y = target_function(new_pt[0])  # Get the y-value from the target function\n",
        "        difference = y - new_pt[1]  # Compute difference from line\n",
        "        X.append(new_pt)\n",
        "        Y.append(difference)\n",
        "\n",
        "    targets = np.sign(Y)  # Convert differences to -1 or 1\n",
        "    return np.array(X), targets\n",
        "\n",
        "def logistic_func(z):\n",
        "    \"\"\"Logistic function.\"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def train_logistic_regression(X, y, learning_rate=0.01, tol=0.01, max_iter=10000):\n",
        "    \"\"\"Train logistic regression using Stochastic Gradient Descent.\"\"\"\n",
        "    weights = np.zeros(X.shape[1])  # Initialize weights\n",
        "    N = len(y)  # Number of samples\n",
        "    epoch_count = 0  # To count the number of epochs\n",
        "\n",
        "    for epoch in range(max_iter):\n",
        "        epoch_count += 1\n",
        "        # Random permutation of indices\n",
        "        indices = np.random.permutation(N)\n",
        "        for i in indices:\n",
        "            z = np.dot(X[i], weights)  # Compute linear combination\n",
        "            prediction = logistic_func(z)  # Apply logistic function\n",
        "            error = y[i] - prediction  # Calculate error\n",
        "            weights += learning_rate * error * X[i]  # Update weights\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(weights) < tol:\n",
        "            break\n",
        "\n",
        "    return epoch_count\n",
        "\n",
        "def main(num_runs=10, N=100):\n",
        "    \"\"\"Run multiple experiments to measure average epochs for convergence.\"\"\"\n",
        "    epoch_counts = []\n",
        "\n",
        "    for _ in range(num_runs):\n",
        "        target_function = generate_line()  # Generate a new target line\n",
        "        X, Y = prepare_data(N, target_function)  # Prepare data\n",
        "        epochs = train_logistic_regression(X, Y)  # Train the model\n",
        "        print(epochs)\n",
        "        epoch_counts.append(epochs)  # Store the epoch count for this run\n",
        "\n",
        "    average_epochs = np.mean(epoch_counts)  # Calculate average epochs\n",
        "    print(f\"Average number of epochs for convergence: {average_epochs:.2f}\")\n",
        "\n",
        "# Run the main function\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "nuDaH4xv2SZj",
        "outputId": "432c24e8-3106-4488-a9e6-abcc245fad45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-d52f8bce147e>:30: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-d52f8bce147e>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-d52f8bce147e>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_runs, N)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mtarget_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Generate a new target line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_logistic_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mepoch_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Store the epoch count for this run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d52f8bce147e>\u001b[0m in \u001b[0;36mtrain_logistic_regression\u001b[0;34m(X, y, learning_rate, tol, max_iter)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Check for convergence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[1;32m   2544\u001b[0m             (ord == 2 and ndim == 1)):\n\u001b[1;32m   2545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2546\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'K'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2547\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m                 \u001b[0mx_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}