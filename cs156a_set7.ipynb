{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRK42nxLXl5o",
        "outputId": "9da8b812-e228-43f8-f421-b0da952dd4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k = 6\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def nonlinear_transform(x1, x2): #function to perform the nonlinear transform\n",
        "  return np.array([1, x1, x2, x1**2, x2**2, x1 * x2, abs(x1 - x2), abs(x1 + x2)])\n",
        "\n",
        "def parse_data(filename): #function to parse the data for each file\n",
        "  data = []\n",
        "  labels = []\n",
        "  with open(filename, 'r') as file: #open the file\n",
        "    for line in file: #for each line of the file get the data x1, x2, y\n",
        "      x1, x2, y = map(float, line.strip().split())\n",
        "      transformed_x = nonlinear_transform(x1, x2) #transform the data\n",
        "      data.append(transformed_x) #append the transformed data to data\n",
        "      labels.append(y) #get the labels\n",
        "  return np.array(data), np.array(labels)\n",
        "\n",
        "def linear_regression(X, y): #perform linear regression to find weights\n",
        "  return np.linalg.pinv(X) @ y\n",
        "\n",
        "def misclassified(y_true, y_pred): #calculate classification error\n",
        "  return np.mean(y_true != np.sign(y_pred))\n",
        "\n",
        "data, labels = parse_data(\"training.txt\") #load in sample data\n",
        "\n",
        "X_train, y_train = data[:25], labels[:25]\n",
        "X_val, y_val = data[25:], labels[25:]\n",
        "\n",
        "best_k = None\n",
        "lowest_error = float('inf')\n",
        "\n",
        "for k in range(3, 8):\n",
        "  X_train_k = X_train[:, :k+1] #use only features up to k\n",
        "  X_val_k = X_val[:, :k+1]\n",
        "\n",
        "  weights = linear_regression(X_train_k, y_train) #train model on training set\n",
        "  y_val_pred = X_val_k @ weights #predict on validation set\n",
        "\n",
        "  error = misclassified(y_val, y_val_pred) #calculate classification error\n",
        "  if error < lowest_error:\n",
        "    lowest_error = error\n",
        "    best_k = k #update best model if error is lower\n",
        "\n",
        "print(f'k = {best_k}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = parse_data(\"testing.txt\") #load out of sample data\n",
        "\n",
        "out_sample_errors = {} #dictionary to store error for each model\n",
        "\n",
        "for k in range(3, 8):\n",
        "    X_train_k = X_train[:, :k+1]\n",
        "    X_test_k = X_test[:, :k+1] #full out-of-sample data for testing\n",
        "\n",
        "    weights = linear_regression(X_train_k, y_train) #train model on the training data\n",
        "    y_test_pred = X_test_k @ weights #predict on the out-of-sample data\n",
        "\n",
        "    out_sample_errors[k] = misclassified(y_test, y_test_pred) #calculate the out-of-sample error\n",
        "\n",
        "best_k_out_sample = min(out_sample_errors, key=out_sample_errors.get)\n",
        "print(f\"k = {best_k_out_sample}\")\n",
        "print(f'error = {min(out_sample_errors.values())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUwzFA1AaKqZ",
        "outputId": "58031d8f-29c1-4dc7-9db7-4f165fa82b73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k = 7\n",
            "error = 0.072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data, labels = parse_data(\"training.txt\") #load in sample data\n",
        "\n",
        "validation_errors = {} #dictionary to store validation errors for each model\n",
        "\n",
        "X_val, y_val = data[:25], labels[:25] #first 25 for validation\n",
        "X_train, y_train = data[25:], labels[25:] #last 10 for training\n",
        "\n",
        "for k in range(3, 8):\n",
        "  X_train_k = X_train[:, :k+1]\n",
        "  X_val_k = X_val[:, :k+1]\n",
        "\n",
        "  weights = linear_regression(X_train_k, y_train) #train on training set\n",
        "  y_val_pred = X_val_k @ weights #prediction on validation set\n",
        "\n",
        "  validation_errors[k] = misclassified(y_val, y_val_pred) #calc classification error\n",
        "\n",
        "best_k = min(validation_errors, key=validation_errors.get)\n",
        "print(f'k = {best_k}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xteAro8dbRo",
        "outputId": "55aa9023-35f2-48b8-d597-595702eaa779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k = 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = parse_data(\"testing.txt\") #load out of sample data\n",
        "\n",
        "out_sample_errors = {} #dictionary to store error for each model\n",
        "\n",
        "for k in range(3, 8):\n",
        "    X_train_k = X_train[:, :k+1]\n",
        "    X_test_k = X_test[:, :k+1] #full out-of-sample data for testing\n",
        "\n",
        "    weights = linear_regression(X_train_k, y_train) #train model on the training data\n",
        "    y_test_pred = X_test_k @ weights #predict on the out-of-sample data\n",
        "\n",
        "    out_sample_errors[k] = misclassified(y_test, y_test_pred) #calculate the out-of-sample error\n",
        "best_k_out_sample = min(out_sample_errors, key=out_sample_errors.get)\n",
        "print(f\"k = {best_k_out_sample}\")\n",
        "print(f'error = {min(out_sample_errors.values())}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEhKR5MGesZ-",
        "outputId": "faadee8f-ed8f-44c1-afbc-e35ffafd752b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k = 6\n",
            "error = 0.192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 10**6 #number of simulations\n",
        "\n",
        "#generate random samples for e1 and e2\n",
        "e1 = np.random.uniform(0, 1, num_samples)\n",
        "e2 = np.random.uniform(0, 1, num_samples)\n",
        "\n",
        "#calculate min(e1, e2) for each pair\n",
        "e = np.minimum(e1, e2)\n",
        "\n",
        "#calculate expected values\n",
        "E_e1 = np.mean(e1)\n",
        "E_e2 = np.mean(e2)\n",
        "E_e = np.mean(e)\n",
        "\n",
        "print(f\"Expected value of e1: {E_e1:.4f}\")\n",
        "print(f\"Expected value of e2: {E_e2:.4f}\")\n",
        "print(f\"Expected value of e: {E_e:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSTaHqmufxgW",
        "outputId": "fea7e673-cae2-49c8-c7a3-c6333c34c8be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected value of e1: 0.5005\n",
            "Expected value of e2: 0.5000\n",
            "Expected value of e: 0.3337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from cvxopt import matrix, solvers\n",
        "import random\n",
        "\n",
        "def generate_target_function():\n",
        "    #generate two random points in [-1, 1] x [-1, 1]\n",
        "    p1 = np.random.uniform(-1, 1, 2)\n",
        "    p2 = np.random.uniform(-1, 1, 2)\n",
        "\n",
        "    #define the line passing through these points\n",
        "    return p1, p2\n",
        "\n",
        "def generate_data(N, f):\n",
        "    #generate random points in [-1, 1] x [-1, 1]\n",
        "    data = np.random.uniform(-1, 1, (N, 2))\n",
        "\n",
        "    #define target labels\n",
        "    p1, p2 = f\n",
        "    labels = np.array([np.sign(np.cross(p2 - p1, x - p1)) for x in data])\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "def pla(data, labels):\n",
        "    w = np.zeros(3)  #dtarting with the zero vector (for w and bias)\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        misclassified = []\n",
        "        for i, x in enumerate(data):\n",
        "            x_augmented = np.concatenate(([1], x))  #with bias term\n",
        "            if np.sign(np.dot(w, x_augmented)) != labels[i]:\n",
        "                misclassified.append(i)\n",
        "\n",
        "        if not misclassified:\n",
        "            break\n",
        "\n",
        "        #pick a random misclassified point\n",
        "        i = random.choice(misclassified)\n",
        "        x_augmented = np.concatenate(([1], data[i]))  #with bias term\n",
        "        w += labels[i] * x_augmented\n",
        "        iteration += 1\n",
        "    return w, iteration\n",
        "\n",
        "def compute_error(f, w, data, labels):\n",
        "    errors = 0\n",
        "    for i, x in enumerate(data):\n",
        "        x_augmented = np.concatenate(([1], x))  #with bias term\n",
        "        if np.sign(np.dot(w, x_augmented)) != labels[i]:\n",
        "            errors += 1\n",
        "    return errors / len(data)\n",
        "\n",
        "def svm(data, labels):\n",
        "    N, d = data.shape\n",
        "    #set up the quadratic programming problem\n",
        "    P = np.eye(d + 1)  #identity matrix of size (d + 1)\n",
        "    P[0, 0] = 0  #no penalty for the bias term\n",
        "    q = np.zeros(d + 1)\n",
        "\n",
        "    #with bias term (add a column of 1s)\n",
        "    X_augmented = np.hstack((np.ones((N, 1)), data))  # N x (d+1)\n",
        "\n",
        "    #create constraint matrix G, each row corresponds to a constraint for a point\n",
        "    G = -np.multiply(labels[:, np.newaxis], X_augmented)  # N x (d+1), each row is -y_n * (1, x_n)\n",
        "\n",
        "    #the vector h contains -1 for each constraint\n",
        "    h = -np.ones(N)\n",
        "\n",
        "    #solve the quadratic programming problem\n",
        "    qp_solution = solvers.qp(matrix(P), matrix(q), matrix(G), matrix(h))\n",
        "    w_svm = np.array(qp_solution['x']).flatten()\n",
        "\n",
        "    #compute the number of support vectors\n",
        "    count_sv = 0\n",
        "    for i, x in enumerate(data):\n",
        "        x_augmented = np.concatenate(([1], x))  #augmented input with bias term\n",
        "        if abs(labels[i] * np.dot(w_svm, x_augmented) - 1) < 0.001:\n",
        "            count_sv += 1\n",
        "\n",
        "    return w_svm, count_sv\n",
        "\n",
        "#run the experiment over multiple runs\n",
        "def run_experiment(N, num_runs):\n",
        "    total_sv = 0\n",
        "    svm_wins = 0\n",
        "    for _ in range(num_runs):\n",
        "        f = generate_target_function()\n",
        "        data, labels = generate_data(N, f)\n",
        "        test_data, test_labels = generate_data(N, f)\n",
        "\n",
        "        # PLA\n",
        "        w_pla, _ = pla(data, labels)\n",
        "        pla_error = compute_error(f, w_pla, test_data, test_labels)\n",
        "\n",
        "        # SVM\n",
        "        w_svm, num_sv = svm(data, labels)\n",
        "        svm_error = compute_error(f, w_svm, test_data, test_labels)\n",
        "        total_sv += num_sv\n",
        "        #check which method has a better error rate\n",
        "        if svm_error <= pla_error:\n",
        "            svm_wins += 1\n",
        "    avg_sv = total_sv / num_runs\n",
        "    svm_percentage = (svm_wins / num_runs) * 100\n",
        "    return svm_percentage, avg_sv\n",
        "\n",
        "svm_percentage, avg_sv = run_experiment(10, 1000)\n",
        "print(f\"SVM wins {svm_percentage:.2f}% of the time\")\n",
        "print(f'avg_sv: {avg_sv}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENzXPAKtog4o",
        "outputId": "c16dd649-5ec4-4245-99e6-242283e3fce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM wins 83.30% of the time\n",
            "avg_sv: 2.558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm_percentage, avg_sv = run_experiment(10, 1000)\n",
        "print(f\"SVM wins {svm_percentage:.2f}% of the time\")\n",
        "print(f'avg_sv: {avg_sv}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-cWvPFRDhew",
        "outputId": "70a4f5b0-b490-43fd-f7f3-fe885d0f0083"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM wins 84.70% of the time\n",
            "avg_sv: 2.584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from cvxopt import matrix, solvers\n",
        "import random\n",
        "\n",
        "# Helper functions\n",
        "def generate_target_function():\n",
        "    # Randomly pick two points\n",
        "    p1 = np.random.uniform(-1, 1, 2)\n",
        "    p2 = np.random.uniform(-1, 1, 2)\n",
        "    slope = (p2[1] - p1[1]) / (p2[0] - p1[0]) if p1[0] != p2[0] else 0\n",
        "    intercept = p1[1] - slope * p1[0]\n",
        "    return lambda x: np.sign(x[1] - (slope * x[0] + intercept))\n",
        "\n",
        "def generate_data(N, f):\n",
        "    data = []\n",
        "    while len(data) < N:\n",
        "        x = np.random.uniform(-1, 1, 2)\n",
        "        y = f(x)\n",
        "        data.append((np.array([1, x[0], x[1]]), y))\n",
        "    return data\n",
        "\n",
        "def pla(X, y, max_iterations=1000):\n",
        "    w = np.zeros(X.shape[1])\n",
        "    for _ in range(max_iterations):\n",
        "        predictions = np.sign(X.dot(w))\n",
        "        misclassified = np.where(predictions != y)[0]\n",
        "        if len(misclassified) == 0:\n",
        "            break\n",
        "        rand_misclassified = random.choice(misclassified)\n",
        "        w += y[rand_misclassified] * X[rand_misclassified]\n",
        "    return w\n",
        "\n",
        "def calculate_disagreement(f, g, num_points=1000):\n",
        "    disagreements = 0\n",
        "    for _ in range(num_points):\n",
        "        x = np.random.uniform(-1, 1, 2)\n",
        "        f_label = f(x)\n",
        "        g_label = np.sign(g([1, x[0], x[1]]))\n",
        "        if f_label != g_label:\n",
        "            disagreements += 1\n",
        "    return disagreements / num_points\n",
        "\n",
        "def svm(data):\n",
        "    X = np.array([x for x, _ in data])\n",
        "    y = np.array([y for _, y in data])\n",
        "\n",
        "    # Set up SVM QP problem\n",
        "    m, n = X.shape\n",
        "    P = matrix(np.outer(y, y) * (X @ X.T))\n",
        "    q = matrix(-np.ones(m))\n",
        "    G = matrix(-np.eye(m))\n",
        "    h = matrix(np.zeros(m))\n",
        "    A = matrix(y.reshape(1, -1), tc='d')\n",
        "    b = matrix(0.0)\n",
        "\n",
        "    # Solve QP\n",
        "    solvers.options['show_progress'] = False\n",
        "    solution = solvers.qp(P, q, G, h, A, b)\n",
        "    alphas = np.ravel(solution['x'])\n",
        "\n",
        "    # Find support vectors\n",
        "    support_vectors = alphas > 1e-5\n",
        "    support_vector_indices = np.where(support_vectors)[0]\n",
        "    support_vectors_count = len(support_vector_indices)\n",
        "\n",
        "    # Compute weights and bias\n",
        "    w = ((alphas * y)[:, None] * X).sum(axis=0)\n",
        "    b = np.mean(y[support_vectors] - X[support_vectors] @ w)\n",
        "\n",
        "    # Define decision function g_SVM\n",
        "    def g_svm(x):\n",
        "        return np.sign(np.dot(w, x) + b)\n",
        "\n",
        "    return g_svm, support_vectors_count\n",
        "\n",
        "def experiment(N=10, runs=1000):\n",
        "    svm_wins = 0\n",
        "    total_support_vectors = 0\n",
        "\n",
        "    for _ in range(runs):\n",
        "        f = generate_target_function()\n",
        "        data = generate_data(N, f)\n",
        "\n",
        "        # Separate X and y\n",
        "        X = np.array([point[0] for point in data])\n",
        "        y = np.array([point[1] for point in data])\n",
        "\n",
        "        # Run PLA\n",
        "        w_pla = pla(X, y)\n",
        "        g_pla = lambda x: np.sign(np.dot(w_pla, x))\n",
        "        pla_disagreement = calculate_disagreement(f, g_pla)\n",
        "\n",
        "        # Run SVM\n",
        "        g_svm, support_vectors_count = svm(data)\n",
        "        svm_disagreement = calculate_disagreement(f, g_svm)\n",
        "        total_support_vectors += support_vectors_count\n",
        "\n",
        "        # Compare errors\n",
        "        if svm_disagreement < pla_disagreement:\n",
        "            svm_wins += 1\n",
        "\n",
        "    svm_win_percentage = (svm_wins / runs) * 100\n",
        "    average_support_vectors = total_support_vectors / runs\n",
        "    print(f\"SVM wins {svm_win_percentage}% of the time.\")\n",
        "    print(f\"Average number of support vectors: {average_support_vectors}\")\n",
        "\n",
        "# Run the experiment\n",
        "experiment()\n",
        "experiment(N=100)\n"
      ],
      "metadata": {
        "id": "qXH67O-qNvrr",
        "outputId": "c8f187bd-e43d-4696-b894-93f3c5ae692c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM wins 54.2% of the time.\n",
            "Average number of support vectors: 2.58\n",
            "SVM wins 56.00000000000001% of the time.\n",
            "Average number of support vectors: 3.102\n"
          ]
        }
      ]
    }
  ]
}